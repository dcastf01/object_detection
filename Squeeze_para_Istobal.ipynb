{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Squeeze para Istobal",
      "provenance": [],
      "collapsed_sections": [
        "wyEZrn0cOIA_",
        "4KFnk27AU5dT",
        "azUwKuOqKkDH",
        "23H2DXV-9Lq8",
        "mOA-FHC_Gm3T"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPVD0RENp5iSTWKWASInlhc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcastf01/object_detection_TFM/blob/main/Squeeze_para_Istobal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiiNJbVrVYQk"
      },
      "source": [
        "Truco Para que no se cuelgue el chrome, se tiene que incluir en la consola del navegador\n",
        "\n",
        "function ClickConnect(){ console.log(\"Working\"); document.querySelector(\"colab-toolbar-button#connect\").click() } setInterval(ClickConnect,60000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOWyOq8pOBBh"
      },
      "source": [
        "#Imports library, dataset and detect device to use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_EikHSgN8Ml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "e56a772f-32ba-459b-c07f-32fcf1017445"
      },
      "source": [
        "#@title Imports library\n",
        "import os, sys, math\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "if 'google.colab' in sys.modules: # Colab-only Tensorflow version selector\n",
        "  %tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3dnrJpKIYZB",
        "cellView": "form"
      },
      "source": [
        "#@title No necesario si no se quiere guardar información\n",
        "use_your_drive = False #@param {type:\"boolean\"}\n",
        "if use_your_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6gTYTcCuZV6"
      },
      "source": [
        "##download dataset from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Bwo0s6rwDo",
        "cellView": "form"
      },
      "source": [
        "#@title Download of dataset\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "# sin_recorte=False #@param {type:\"boolean\"} \n",
        "con_recorte=True #@param {type:\"boolean\"}  \n",
        "# con_recorte_y_filtro_frontales=False #@param {type:\"boolean\"}  \n",
        "if con_recorte:\n",
        "  file_id = '1MKjn9qLc2o82Vk1Rua-7TTOmxrOAWi6U'\n",
        "# if con_recorte_y_filtro_frontales:\n",
        "#   file_id=\"13rqJpDl3YKI0oHM0HHwVdifKEw6WP2IX\"\n",
        "# if sin_recorte:\n",
        "#   file_id=\"1NiZGvXHs9aQpbciQHwdqsCXJTbbizJap\"\n",
        "\n",
        "destination = '/content/file.zip'\n",
        "\n",
        "download_file_from_google_drive(file_id, destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPT9_bRAud1h",
        "cellView": "form"
      },
      "source": [
        "#@title Unzip file\n",
        "%%capture\n",
        "!unzip file.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVX_kfp4N815",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "054128ce-77df-4248-a493-6e6fc060c17e"
      },
      "source": [
        "#@title Detect hardware\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "except ValueError:\n",
        "  tpu = None\n",
        "  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
        "    \n",
        "# Select appropriate distribution strategy for hardware\n",
        "if tpu:\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "  print('Running on TPU ', tpu.master())  \n",
        "elif len(gpus) > 0:\n",
        "  #strategy = tf.distribute.MirroredStrategy(gpus) # this works for 1 to multiple GPUs\n",
        "  print('Running on ', len(gpus), ' GPU(s) ')\n",
        "  strategy=tf.distribute.OneDeviceStrategy(gpus[0])\n",
        "else:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on CPU')\n",
        "\n",
        "# How many accelerators do we have ?\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on CPU\n",
            "Number of accelerators:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8uH-5C5ni-7"
      },
      "source": [
        "comprobando porque no va la gpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylMsJwu4QrwD"
      },
      "source": [
        "#Generación del modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuB_ctzLQ5-F",
        "cellView": "form"
      },
      "source": [
        "#@title Insert setup\n",
        "import numpy as np\n",
        "import pathlib\n",
        "\n",
        "##esto pasarlo al config\n",
        "BATCH_SIZE = 32 #normalmente en 32 \n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "nb_epochs=100\n",
        "#dict_data=diccionario_con_datos(\"squeeze_con_imagenes_en_crudo\")\n",
        "Directory=\"/content/dataset\"\n",
        "con_recorte=True #@param {type:\"boolean\"}  \n",
        "con_batch=True #@param {type:\"boolean\"}  \n",
        "con_dropout=False #@param {type:\"boolean\"} \n",
        "if con_recorte:\n",
        "  Nombre_modelo=\"Squeeze_con_imagenes_con_recortes\"\n",
        "  if con_batch:\n",
        "    Nombre_modelo=\"Squeeze_con_imagenes_con_recortes_filtro_y_Batch\"\n",
        "    if con_dropout:\n",
        "      value_dropout=0.3 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "      Nombre_modelo=\"Squeeze_con_imagenes_con_recortes_Batch_y_Dropout_solo_sinBypass\"+str(value_dropout)\n",
        "if not con_recorte:\n",
        "  Nombre_modelo=\"Squeeze_con_imagenes_sin_recortes\"\n",
        "  if not con_batch or not con_dropout:\n",
        "    Nombre_modelo=\"algo raro\"\n",
        "new_metric=True #@param {type:\"boolean\"}  \n",
        "if new_metric:\n",
        "  Nombre_modelo+=\"read_metrics\"\n",
        "\n",
        "#Nombre_modelo=dict_data[\"Nombre_modelo\"]\n",
        "\n",
        "data_dir = pathlib.Path(Directory)\n",
        "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n",
        "print(CLASS_NAMES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYQpQ0_BRcOm",
        "cellView": "form"
      },
      "source": [
        "#@title Create a dataset\n",
        "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
        "                                                                shear_range=0.2,\n",
        "                                                                zoom_range=0.2,\n",
        "                                                               horizontal_flip=True,\n",
        "                                                                validation_split=0.2)\n",
        "\n",
        "\n",
        "train_generator  = image_generator.flow_from_directory(directory=str(Directory),\n",
        "                                                     batch_size=BATCH_SIZE,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     classes = list(CLASS_NAMES),\n",
        "                                                      class_mode=\"categorical\",\n",
        "                                                    \n",
        "                                                     subset=\"training\"                                                     \n",
        "                                                     )\n",
        "validation_generator  = image_generator.flow_from_directory(directory=str(Directory),\n",
        "                                                     batch_size=BATCH_SIZE,\n",
        "                                                     shuffle=True,\n",
        "                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                     classes = list(CLASS_NAMES),\n",
        "                                                      class_mode=\"categorical\",\n",
        "                                                     subset=\"validation\"                                                     \n",
        "                                                     )\n",
        "print(\"number of images en train\",train_generator.n)\n",
        "print(\"number of images en validation\",validation_generator.n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyEZrn0cOIA_"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjowu3UX_HLh",
        "cellView": "form"
      },
      "source": [
        "#@title Generate model\n",
        "\n",
        "IMAGE_SIZE=[227,227] #en el experimento usaron 227\n",
        "NUM_CLASS=train_generator.num_classes\n",
        "with strategy.scope(): # this line is all that is needed to run on TPU (or multi-GPU, ...)\n",
        "\n",
        "  bnmomemtum=0.9\n",
        "  def fire(x, squeeze, expand):\n",
        "   \n",
        "    if con_batch:\n",
        "      x=tf.keras.layers.BatchNormalization()(x)\n",
        "    if con_dropout:\n",
        "      x=tf.keras.layers.Dropout(value_dropout)(x)\n",
        "    y  = tf.keras.layers.Conv2D(filters=squeeze, kernel_size=1, activation='relu', padding='same')(x)\n",
        "    \n",
        "    y1 = tf.keras.layers.Conv2D(filters=expand, kernel_size=1, activation='relu', padding='same')(y)\n",
        " \n",
        "    y3 = tf.keras.layers.Conv2D(filters=expand, kernel_size=3, activation='relu', padding='same')(y)\n",
        "   \n",
        "   \n",
        "    return tf.keras.layers.concatenate([y1, y3])\n",
        "\n",
        "  def fire_module(squeeze, expand):\n",
        "    return lambda x: fire(x, squeeze, expand)\n",
        "    \n",
        "  def fire_with_bypass(x, squeeze, expand):\n",
        "    \n",
        "    if con_batch:\n",
        "      x=tf.keras.layers.BatchNormalization()(x)\n",
        "    #if con_dropout:\n",
        "     # x=tf.keras.layers.Dropout(value_dropout)(x)\n",
        "    y  = tf.keras.layers.Conv2D(filters=squeeze, kernel_size=1, activation='relu', padding='same')(x)\n",
        "\n",
        "    y1 = tf.keras.layers.Conv2D(filters=expand, kernel_size=1, activation='relu', padding='same')(y)\n",
        " \n",
        "    y3 = tf.keras.layers.Conv2D(filters=expand, kernel_size=3, activation='relu', padding='same')(y)\n",
        "   \n",
        "    y5=tf.keras.layers.concatenate([y1, y3])\n",
        "    return tf.keras.layers.add([x, y5])\n",
        "\n",
        "  def fire_module_with_bypass(squeeze, expand):\n",
        "    return lambda x: fire_with_bypass(x, squeeze, expand)\n",
        " \n",
        "  x = tf.keras.layers.Input(shape=[*IMAGE_SIZE, 3]) # input is 192x192 pixels RGB\n",
        "\n",
        "  y = tf.keras.layers.Conv2D(kernel_size=3, filters=95,strides=2, use_bias=True, activation='relu')(x)\n",
        "  y = tf.keras.layers.MaxPooling2D(pool_size=2)(y)\n",
        "  y = fire_module(16, 64)(y)\n",
        "  y = fire_module_with_bypass(16, 64)(y)\n",
        "  y = fire_module(32, 128)(y)\n",
        "  y = tf.keras.layers.MaxPooling2D(pool_size=2)(y)\n",
        "  y = fire_module_with_bypass(32, 128)(y)\n",
        "  y = fire_module(48, 192)(y)\n",
        "  y = fire_module_with_bypass(48, 192)(y)\n",
        "  y = fire_module(64, 256)(y)\n",
        "  y = tf.keras.layers.MaxPooling2D(pool_size=2)(y)\n",
        "  y = fire_module_with_bypass(64, 256)(y)\n",
        "  \n",
        "  if con_batch:\n",
        "    y=tf.keras.layers.BatchNormalization()(y)\n",
        "  if con_dropout:\n",
        "      y=tf.keras.layers.Dropout(value_dropout)(y)\n",
        "  y = tf.keras.layers.Conv2D(kernel_size=3, filters=NUM_CLASS,strides=2, use_bias=True, activation='relu')(y) #en el modelo el numero de filtros es igual al número de clases\n",
        "  y = tf.keras.layers.GlobalAveragePooling2D()(y)\n",
        "  y = tf.keras.layers.Dense(NUM_CLASS, activation='softmax')(y) \n",
        "\n",
        "  #revisar para disminuir el número de clases\n",
        "\n",
        "\n",
        "  model = tf.keras.Model(x, y)\n",
        " # initiate RMSprop optimizer\n",
        "  opt = tf.keras.optimizers.RMSprop(learning_rate=0.0005, decay=1e-6)\n",
        "\n",
        "  METRICS=[ \n",
        "      \n",
        "      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "      tf.keras.metrics.Accuracy(name=\"accuracy_binary\"),\n",
        "      tf.keras.metrics.TruePositives(name='tp'),\n",
        "      tf.keras.metrics.FalsePositives(name='fp'),\n",
        "      tf.keras.metrics.TrueNegatives(name='tn'),\n",
        "      tf.keras.metrics.FalseNegatives(name='fn'), \n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall'),\n",
        "      tf.keras.metrics.AUC(name='auc',num_thresholds=500),#cambiar el auc\n",
        "      tf.keras.metrics.TopKCategoricalAccuracy( k=3, name='top_3_categorical_accuracy'),\n",
        "      tf.keras.metrics.TopKCategoricalAccuracy( k=5, name='top_5_categorical_accuracy'),\n",
        "      tf.keras.metrics.TopKCategoricalAccuracy( k=10, name='top_10_categorical_accuracy')\n",
        "        ]\n",
        "    # Let's train the model using RMSprop\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=opt,\n",
        "                metrics=METRICS)\n",
        " #model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1azRWkfOMXz",
        "cellView": "form"
      },
      "source": [
        "#@title if you execute the cell you have a plot of model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiHTrbeBrTPy",
        "cellView": "form"
      },
      "source": [
        "#@title create different callbacks\n",
        "#la variable model_dir tiene que terminar con \"/\"\n",
        "\n",
        "model_dir = \"/content/tensorboard/\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "def compilacion(model,Nombre_modelo=\"faltonombre\"):\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras.optimizers import SGD\n",
        "    \n",
        "   \n",
        "    from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "    from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "   \n",
        "    import datetime\n",
        "    verbose=1\n",
        "    patience = 50\n",
        "\n",
        "    #log_dir=\"logs/\"+Nombre_modelo+\"/tensorboard/\"+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    log_dir=model_dir+\"logs/\"+Nombre_modelo+\"/tensorboard\"\n",
        "    tensor_board = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_images=True)\n",
        "    log_file_path = model_dir+'logs/'+Nombre_modelo+'/history.log'\n",
        "    csv_logger = CSVLogger(log_file_path, append=True)\n",
        "    early_stop = EarlyStopping('val_accuracy', patience=patience)\n",
        "    reduce_lr = ReduceLROnPlateau('val_accuracy', factor=0.1, patience=int(patience / 4), verbose=1)\n",
        "    trained_models_path = model_dir+'models/'\n",
        "    model_names = trained_models_path + Nombre_modelo+\"/\"+'primerTest{epoch:02d}val_accuracy{val_accuracy:.2f}time'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'.hdf5'\n",
        "    model_checkpoint = ModelCheckpoint(model_names, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "    callbacks = [tensor_board, model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
        "    \n",
        "    return callbacks\n",
        "callbacks=compilacion(model,Nombre_modelo)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn6x3AXJFSQi",
        "cellView": "form"
      },
      "source": [
        "#@title load model pre-trained\n",
        "#usar solo si se dispone de alguna versióon del modelo entrenada\n",
        "use_model_pre_trained = False #@param {type:\"boolean\"}\n",
        "\n",
        "if use_model_pre_trained:\n",
        "  model_pretraine_hdf5=\"\"#@param {type:\"string\"}\n",
        "  model.load_weights(\"/content/drive/My Drive/AI/Computer vision/car recognition/framework_label/models/Squeeze_imagenes_en_crudo/primerTest52val_accuracy0.46time20200326-081652.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KFnk27AU5dT"
      },
      "source": [
        "#Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHbIIGzwRymq",
        "cellView": "form"
      },
      "source": [
        "#@title train model\n",
        "history=model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch =( train_generator.samples // BATCH_SIZE),\n",
        "    validation_data = validation_generator, \n",
        "    validation_steps = (validation_generator.samples // BATCH_SIZE),\n",
        "    epochs = 100,\n",
        "    callbacks=callbacks\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azUwKuOqKkDH"
      },
      "source": [
        "#tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMqKiuzmKlpu"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVnMhwg7nJfz"
      },
      "source": [
        "model_tensorboard=model_dir+\"/logs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WljBT_1aK0dt"
      },
      "source": [
        "%tensorboard --logdir {model_tensorboard}\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}